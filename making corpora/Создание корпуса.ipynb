{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5e94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import sqlite3\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c89790",
   "metadata": {},
   "source": [
    "# 1. Парсер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4b9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab33b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('panorama_news.db') # создаю базу данных, где будут храниться тексты и их метаиформация\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS news \n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, topic, link, title, date, text)\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def db_write(id_text, topic, link, title, date, text): # функция, которая записывает нужную информацию в базу данных\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO news \n",
    "            (id, topic, link, title, date, text) \n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            id_text, topic, link, title, date, text)\n",
    "    )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ba5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_text = 0\n",
    "parsed = []\n",
    "def parse_review(review, topic): #парсинг отдельного текста\n",
    "    global id_text\n",
    "    url = 'https://panorama.pub' + review.attrs['href']  #ссылка на текст\n",
    "    req = driver.get(url)\n",
    "    page = driver.page_source\n",
    "    if len(page) > 0:\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        title = soup.find('h1', {'class': 'font-bold text-2xl md:text-3xl lg:text-4xl pl-1 pr-2 self-center'}).text.strip('\\n')\n",
    "        if title not in parsed:\n",
    "            metainfo = soup.find('div', {'class': 'flex flex-col gap-x-3 gap-y-1.5 flex-wrap sm:flex-row'}).text\n",
    "            if 'сегодня' in metainfo:\n",
    "                date = '16 окт. 2022 г.'\n",
    "            elif 'вчера' in metainfo:\n",
    "                date = '15 окт. 2022 г.'\n",
    "            elif 'позавчера' in metainfo:\n",
    "                date = '14 окт. 2022 г.'\n",
    "            else:\n",
    "                date = re.findall(r'[1-3]?\\d \\w{2,4}\\.? \\d{4} г.', metainfo)[0]\n",
    "            text = soup.find('div', {'class': 'entry-contents pr-0 md:pr-8'}).text.replace('\\n', ' ') # текст\n",
    "            db_write(id_text, topic, url, title, date, text) # записываю в базу данных\n",
    "            parsed.append(title)\n",
    "            id_text += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db122a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(link, topic): #парсинг страницы с текстами\n",
    "    time.sleep(2)\n",
    "    req = driver.get(link)\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    reviews = soup.find_all('a', {'class': 'flex flex-col rounded-md hover:text-secondary hover:bg-accent/[.1] mb-2'}) #нахожу отдельные тексты на странице\n",
    "    for review in reviews:\n",
    "        time.sleep(2)\n",
    "        parse_review(review, topic) #отправляю текст на парсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d098f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,47):\n",
    "    url = f'https://panorama.pub/science?page={i}'\n",
    "    parse_page(url, 'Наука')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f06d6",
   "metadata": {},
   "source": [
    "# 2. Анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1edb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import sqlite3\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc5d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer() # анализирую с помощью пайморфи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755e8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbfdf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('panorama_news.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d3fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "id_text = 0\n",
    "for text in cur.execute('SELECT * FROM news'): # из базы данных прохожусь по каждому тексту\n",
    "    sentences = []\n",
    "    word_count = 0\n",
    "    for sent in sent_tokenize(text[5]): # делю каждый текст на предложения\n",
    "        words = []\n",
    "        for word in casual_tokenize(sent, preserve_case=False): # каждое предложение делю на слова\n",
    "            if word.isalpha() or all(ord(i) in range(ord('А'), ord('я')+1) or ord(i) in (ord('Ё'), ord('ё'), ord('-')) for i in word): # слова, написанные буквами анализирую\n",
    "                pos = []\n",
    "                lemma = []\n",
    "                for analysis in m.parse(word): # добавляю все возможные леммы и пос-тэги\n",
    "                    pos.append(analysis.tag.POS)\n",
    "                    lemma.append(analysis.normal_form)\n",
    "                dct_word = {'word': word, 'POS': pos, 'lemma': lemma} \n",
    "                # создаю словарь анализа слова с самим словом, всеми возможными его пос-тэгами и леммами\n",
    "                words.append(dct_word) # добавляю словарь в список слов текущего предложения\n",
    "                word_count += 1\n",
    "            elif word.isnumeric():\n",
    "                dct_word = {'word': word, 'POS': ['NUMR'], 'lemma': [word]}\n",
    "                words.append(dct_word) # добавляю словарь в список слов текущего предложения\n",
    "                word_count += 1\n",
    "        dct_sent = {'sentence': sent, 'words': words}\n",
    "        # создаю словарь предложения с самим предложением и списком всех слов в нём\n",
    "        sentences.append(dct_sent) # добавляю его в список предложений текущего текста\n",
    "    if word_count >= 100: \n",
    "        # если в тексте насчиталось больше 100 словоформ, создаю его словарь с метаинформацией и добавляю в общий список текстов\n",
    "        dct_text = {'id': id_text, 'link': text[2], 'date': text[4], 'title': text[3], 'sentences': sentences}\n",
    "        id_text += 1\n",
    "        texts.append(dct_text)\n",
    "    if len(texts) >= 100: # как только набралось 100 текстов, останавливаюсь\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69ca836",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('panorama_corpus.json', 'w', encoding='utf-8') as f: # записываю корпус в json\n",
    "    json.dump(texts, f, ensure_ascii=False, indent='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4e0f6",
   "metadata": {},
   "source": [
    "Ниже представлена структура корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {'id': id_text,\n",
    "     'link': link,\n",
    "     'date': date,\n",
    "     'title': title,\n",
    "     'sentences':\n",
    "         [\n",
    "             {'sentence': text,\n",
    "              'words':\n",
    "                  [\n",
    "                      {'word': token,\n",
    "                       'POS':\n",
    "                           ['POS_1', 'POS_2', ...]\n",
    "                        'lemma':\n",
    "                           ['lemma_1', 'lemma_2', ...]\n",
    "                      },\n",
    "                      # word_2, word_3, ...\n",
    "                  ]\n",
    "             },\n",
    "             # sentence_2, sentence_3, ...\n",
    "         ]\n",
    "    },\n",
    "    # text_2, text_3, ...\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
